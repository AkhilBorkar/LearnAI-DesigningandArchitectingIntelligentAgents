{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.1: Integration with Cognitive Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this session, we will break up making bots smarter into three sections:\n",
    "1.  Cognitive Services\n",
    "2.  Adding Cognitive Services to bots\n",
    "3.  Other ways to make bots smarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 1: Cognitive Services \n",
    "Cognitive Services can be used to infuse your apps, websites and [bots with algorithms](https://docs.microsoft.com/en-us/azure/bot-service/bot-service-concept-intelligence) to see, hear, speak, understand, and interpret your user needs through natural methods of communication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are five main categories for the available Cognitive Services:\n",
    "- **Language understanding**: Allow your apps to process natural language with pre-built scripts, evaluate sentiment and learn how to recognize what users want\n",
    "- **Knowledge extraction**: Map complex information and data to solve tasks such as intelligent recommendations and semantic search\n",
    "- **Speech recognition and conversion**: Convert spoken audio into text, use voice for verification, or add speaker recognition to your app\n",
    "- **Web search**: Add Bing Search APIs to your apps and harness the ability to comb billions of webpages, images, videos, and news with a single API call\n",
    "- **Image and video understanding**: Image-processing algorithms to identify, caption and moderate your pictures and videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can browse all the specific APIs in the [Services Directory](https://azure.microsoft.com/en-us/services/cognitive-services/directory/). \n",
    "\n",
    "Next, we're going to dive a bit deeper (but still relatively on the surface - check out the [documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/Welcome) for technical details) - and provide something similar to a \"cheat sheet\" that includes a very brief each of the Cognitive Services. We'll tell you the main things they do and what to look out for (i.e. are they going away, only projects at the moment, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 1.1: Language understanding\n",
    "- [Language Understanding Intelligent Service (LUIS)](https://azure.microsoft.com/en-us/services/cognitive-services/language-understanding-intelligent-service/)\n",
    "    - Extract intents and entities from natural language\n",
    "- [Text Analytics API](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/)\n",
    "    - Determine language, key phrases, and sentiment\n",
    "- [Bing Spell Check API](https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/)\n",
    "    - Correct spelling errors, recognize name differences, brands, slang, and homonyms\n",
    "- [Linguistic Analysis API (preview)](https://azure.microsoft.com/en-us/services/cognitive-services/linguistic-analysis-api/)\n",
    "    - Text structure analyses – _being moved to [text analytics modules](https://docs.microsoft.com/en-us/azure/machine-learning/studio/text-analytics-module-tutorial) in AML Studio_\n",
    "- [Web Language Model API (preview)](https://azure.microsoft.com/en-us/services/cognitive-services/web-language-model/)\n",
    "    - Word breaking – _being moved to [text analytics modules](https://docs.microsoft.com/en-us/azure/machine-learning/studio/text-analytics-module-tutorial) in AML Studio_ \n",
    "- [Translator Text API](https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/)\n",
    "    - Real-time text translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 1.2: Knowledge extraction\n",
    "- [Project Entity Linking](https://labs.cognitive.microsoft.com/en-us/project-entity-linking)\n",
    "    - Disambiguates entities from context and links text from your bot to additional information – _project/labs_\n",
    "- [Project Knowledge Exploration](https://labs.cognitive.microsoft.com/en-us/project-knowledge-exploration)\n",
    "    - Quickly add interactive search to structured data – _project/labs_\n",
    "- [Project Academic Knowledge](https://labs.cognitive.microsoft.com/en-us/project-academic-knowledge)\n",
    "    - Help users find the academic resources they’re looking for – _project/labs_\n",
    "- [QnA Maker API (preview)](https://qnamaker.ai/)\n",
    "    - Quickly and easily create simple question and answer bots based on FAQ URLs, structured documents, or editorial content\n",
    "- [Custom Decision Service (preview)](https://azure.microsoft.com/en-us/services/cognitive-services/custom-decision-service/)\n",
    "    - Use reinforcement learning in a new approach for personalizing content\n",
    "> \"Projects/labs provide early adopters with an early look at emerging Cognitive Services technologies. These technologies are still under development and do not have the same performance quality as generally available APIs, but don't let that stop you from discovering and trying the newest Cognitive Services technologies. Please try these new technologies and let us know what you think.” –Cognitive Services Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 1.3: Speech recognition and conversion\n",
    "- [Bing Speech API](https://azure.microsoft.com/en-us/services/cognitive-services/speech/)\n",
    "    - Convert speech-to-text and text-to-speech, understand intent\n",
    "- [Custom Speech Service (CRIS) (preview)](https://azure.microsoft.com/en-us/services/cognitive-services/custom-speech-service/)\n",
    "    - Overcome speech recognition barriers such as speaking style, vocabulary, and background noise\n",
    "- [Speaker Recognition API (preview)](https://azure.microsoft.com/en-us/services/cognitive-services/speaker-recognition/)\n",
    "    - Identify individual speakers or use speech as a means of authentication\n",
    "- [Translator Speech API](https://azure.microsoft.com/en-us/services/cognitive-services/translator-speech-api/)\n",
    "    - Easily add speech translation to your bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 1.4: Web search\n",
    "- [Bing Web Search API](https://azure.microsoft.com/en-us/services/cognitive-services/bing-web-search-api/)\n",
    "    - Conduct web searches within bots\n",
    "- [Bing Image Search API](https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/)\n",
    "    - Conduct image searches within bots\n",
    "- [Bing Video Search API](https://azure.microsoft.com/en-us/services/cognitive-services/bing-video-search-api/)\n",
    "    - Conduct video searches within bots\n",
    "- [Bing News Search API](https://azure.microsoft.com/en-us/services/cognitive-services/bing-news-search-api/)\n",
    "    - Conduct news searches within bots\n",
    "- [Bing Autosuggest API](https://azure.microsoft.com/en-us/services/cognitive-services/autosuggest/)\n",
    "    - Automated search suggestions – _probably not applicable for a standard bot_\n",
    "- [Bing Entity Search API](https://azure.microsoft.com/en-us/services/cognitive-services/bing-entity-search-api/)\n",
    "    - Bring rich context about entities (like people, places, things, etc.)\n",
    "- [Bing Custom Search API](https://azure.microsoft.com/en-us/services/cognitive-services/bing-custom-search/)\n",
    "    - Easy-to-use, ad-free, commercial-grade search tool that lets you deliver the results you want\n",
    "- [Project Local Insights](https://labs.cognitive.microsoft.com/en-us/project-local-insights)\n",
    "    - Score the attractiveness of a location - _project/labs_\n",
    "- [Project Event Tracking](https://labs.cognitive.microsoft.com/en-us/project-event-tracking)\n",
    "    - Find events associated with Wikipedia entities - _project/labs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 1.5: Image and video understanding\n",
    "- [Computer Vision API](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/)\n",
    "    - Obtain tags, descriptions, adult/racy scores, text, hand writing, celebrities, and more from images\n",
    "- [Face API](https://azure.microsoft.com/en-us/services/cognitive-services/face/)\n",
    "    - Detect faces and compare similar ones, organize similar images in groups, identify previously tagged people in images\n",
    "- [Video indexer (preview)](https://azure.microsoft.com/en-us/services/cognitive-services/video-indexer/)\n",
    "    - Unlock video insights – _Video API ended, Video Indexer replaced it_\n",
    "- [Custom Vision Service](https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/)\n",
    "    - Custom image classification models made easy\n",
    "- [Content Moderator](https://azure.microsoft.com/en-us/services/cognitive-services/content-moderator/)  \n",
    "    - Machine-assisted moderation of text and images\n",
    "- [Project Gesture](https://labs.cognitive.microsoft.com/en-us/project-gesture)\n",
    "    - Incorporate gesture-based controls into your apps - _project/labs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 2: Adding Cognitive Services to Bots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's assume you've completed all the recommended steps in the course up until now for a project you're working on. You've designed your bot, determined your LUIS schema, created an architecture, and picked Cognitive Services to include. Your next task is going to be to put it all together and actually build the bot.  \n",
    "\n",
    "This is not a coding course, so we will not spend a lot of time here. We do want to share a few examples of how you _might_ integrate some of the various services into a bot. Our examples will show code from the .NET BotBuilder SDK ([v3](https://github.com/Microsoft/BotBuilder/tree/master/CSharp) and [v4](https://github.com/Microsoft/botbuilder-dotnet))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The examples we will walk through will show how to integrate QnA Maker, LUIS, Azure Search, and Computer Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 2.1: QnA Maker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Integrating QnA Maker into a bot is simple with the v4 SDK. We'll highlight some of the code from [this sample](https://github.com/Microsoft/botbuilder-dotnet/tree/master/samples/Microsoft.Bot.Samples.Ai.QnA). If you'll be integrating this into a bot, it is quick, simple, and recommended to [build the SDK](https://github.com/Microsoft/botbuilder-dotnet/wiki/Building-the-SDK) and test out the sample yourself.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The basic shell of your bot is [standard](https://github.com/Microsoft/botbuilder-dotnet/wiki#getting-started). To be able to access the QnA service, the first thing we'll want to do is update our Startup.cs file to configure the service. We can do this by updating `ConfigureServices` to the following:\n",
    "```csharp\n",
    " public void ConfigureServices(IServiceCollection services)\n",
    "        {\n",
    "            services.AddBot<QnAMakerBot>(options =>\n",
    "            {\n",
    "                options.CredentialProvider = new \t\t\t\n",
    "\t\t\tConfigurationCredentialProvider(Configuration);\n",
    "                var qnaOptions = new QnAMakerMiddlewareOptions\n",
    "                {\n",
    "                    SubscriptionKey = \"xxxxxx\",\n",
    "                    KnowledgeBaseId = \"xxxxxx\"\n",
    "                };\n",
    "                var middleware = options.Middleware;\n",
    "                middleware.Add(new QnAMakerMiddleware(qnaOptions));\n",
    "            });\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By updating, we've added the information related to our QnA Maker instance, and we've [added the middleware](https://github.com/Microsoft/botbuilder-dotnet/wiki/Creating-Middleware) to call the service.\n",
    "\n",
    "Finally, we create the main class for the bot we configured, called QnAMakerBot.cs. This example is very simple. We call the service with the user's request, and if there is a match, the service sends it to the user. If there is not a match or the user has just joined the conversation, you can see how we respond. Make sure you understand the code below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```csharp\n",
    "using System.Threading.Tasks;\n",
    "using Microsoft.Bot.Builder;\n",
    "using Microsoft.Bot.Schema;\n",
    "\n",
    "namespace Microsoft.Bot.Samples.Ai.QnA\n",
    "    {\n",
    "        public class QnAMakerBot : IBot\n",
    "        {\n",
    "            public QnAMakerBot() { }\n",
    "            public async Task OnReceiveActivity(ITurnContext context)\n",
    "            {\n",
    "                switch (context.Activity.Type)\n",
    "                {\n",
    "                    case ActivityTypes.Message:\n",
    "                        if (context.Activity.Type == ActivityTypes.Message && context.Responded == false)\n",
    "                        {\n",
    "                            await context.SendActivity(\"No good match found in the KB.\");\n",
    "                        }\n",
    "                        break;\n",
    "                    case ActivityTypes.ConversationUpdate:\n",
    "                        foreach (var newMember in context.Activity.MembersAdded)\n",
    "                        {\n",
    "                            if (newMember.Id != context.Activity.Recipient.Id)\n",
    "                            {\n",
    "                                await context.SendActivity(\"Hello and welcome to the QnA Maker Sample bot.\");\n",
    "                            }\n",
    "                        }\n",
    "                        break;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 2.2: LUIS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adding LUIS is similar to adding QnA Maker in the sense that we also use `ConfigureServices` to reference our service and add middleware to call it. In Startup.cs, we would update `ConfigureServices` to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```csharp\n",
    " // This method gets called by the runtime. Use this method to add services to the container.\n",
    "        public void ConfigureServices(IServiceCollection services)\n",
    "        {\n",
    "            services.AddSingleton(_ => Configuration);\n",
    "            services.AddBot<PictureBot>(options =>\n",
    "            {\n",
    "                options.CredentialProvider = new ConfigurationCredentialProvider(Configuration);\n",
    "                var middleware = options.Middleware;\n",
    "\n",
    "                middleware.Add(new UserState<UserData>(new MemoryStorage()));\n",
    "                middleware.Add(new ConversationState<ConversationData>(new MemoryStorage()));\n",
    "                // Add Regex ability below\n",
    "                middleware.Add(new RegExpRecognizerMiddleware()\n",
    "                    .AddIntent(\"search\", new Regex(\"search picture(?:s)*(.*)\", RegexOptions.IgnoreCase))\n",
    "                    .AddIntent(\"share\", new Regex(\"share picture(?:s)\", RegexOptions.IgnoreCase))\n",
    "                    .AddIntent(\"order\", new Regex(\"order picture(?:s)*(.*)\", RegexOptions.IgnoreCase))\n",
    "                    .AddIntent(\"help\", new Regex(\"help(.*)\", RegexOptions.IgnoreCase)));\n",
    "                // Add LUIS ability below\n",
    "                middleware.Add(new LuisRecognizerMiddleware(\n",
    "                    new LuisModel(“AppId\", “AppPassword\", new Uri(“LuisUri\"))));\n",
    "            });\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can see in this sample we are adding other middleware as well. We're using `UserState` and `ConversationState` to manage the context of our conversations.  \n",
    "\n",
    "You might also notice we're adding `RegExRecognizerMiddleware` with intents of \"search,\" \"share,\" \"order,\" and \"help.\" For this example, we're creating a picture bot that can search, share, and order pictures. The reason we call RegEx first is to limit our calls to the LUIS service (optimizing cost and latency). For example, if a user says, \"search pictures,\" we can take them to the case for that without calling LUIS. But, if the user says, \"I'm looking to find a few pictures,\" we can call LUIS to determine what the user's intent is. \n",
    "\n",
    "This example also involves organizing the code via [Topics](https://github.com/Microsoft/botbuilder-dotnet/blob/master/samples/AlarmBot/README.md). You don't need to understand Topics to understand the sample, but you can read more about them [here](https://github.com/Microsoft/botbuilder-dotnet/blob/master/samples/AlarmBot/README.md), and we'll mention them again in the next section.  \n",
    "\n",
    "In our default topic, after we've confirmed we've greeted the user, we will handle their incoming messages as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```csharp\n",
    "public async Task<bool> ContinueTopic(ITurnContext context)\n",
    "        {\n",
    "            var conversation = ConversationState<ConversationData>.Get(context);\n",
    "            var recognizedIntents = context.Services.Get<IRecognizedIntents>();\n",
    "\n",
    "            switch (context.Activity.Type)\n",
    "            {\n",
    "                case ActivityTypes.Message:\n",
    "                    switch (recognizedIntents.TopIntent?.Name)\n",
    "                    {\n",
    "                        case \"search\": //add search topic\n",
    "                        case \"share\": //add share topic\n",
    "                        case \"order\": //add order topic\n",
    "                        case \"help\": //add help topic\n",
    "                        default:\n",
    "                            // adding app logic when Regex doesn't find an intent - consult LUIS\n",
    "                            var result = \t\t\t\tcontext.Services.Get<RecognizerResult>(LuisRecognizerMiddleware.LuisRecognizerResultKey);\n",
    "                            var topIntent = result?.GetTopScoringIntent();\n",
    "\n",
    "                            switch ((topIntent != null) ? topIntent.Value.key : null)\n",
    "                            {\n",
    "                                case null: //add logic when there's no result\n",
    "                                case \"None\": //add none logic\n",
    "                                case \"Help\": //add help topic\n",
    "                                case \"OrderPic\": //add order topic\n",
    "                                case \"SharePic\": //add share topic\n",
    "                                case \"SearchPics\": //add search topic\n",
    "                                default: //add default action\n",
    "                            }\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to Startup.cs, we are first checking if RegEx came back with any recognized intents. If it does, we branch off, but if it doesn't, we call LUIS and branch off accordingly.  \n",
    "\n",
    "You can see the full lab for building this bot [here](https://github.com/Azure/LearnAI-Bootcamp/tree/master/lab02.2-building_bots) and the full bootcamp for Emerging AI Developers [here](https://github.com/Azure/LearnAI-Bootcamp/blob/master/emergingaidev_bootcamp.md). You can also read more about [middleware and the architecture for the v4 SDK](https://github.com/Microsoft/botbuilder-dotnet/wiki/Overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 2.3: Azure Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For Azure Search, we will continue with the previous example of developing a picture bot, focusing on how to add search to the bot.\n",
    "\n",
    "Within the topic, you'll have to add three methods: `StartAsync`, `CreateSearchIndexClient`, and `SendResultsAsync`. \n",
    "\n",
    "In `StartAsync`, we'll call `CreateSearchIndexClient` to fill in our search service information, call the search service with the search request from our user, and call `SendResultsAsync` to create a response to send back to the user.\n",
    "\n",
    "Review the sample code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```csharp\n",
    "public async Task StartAsync(ITurnContext context)\n",
    "        {\n",
    "            ISearchIndexClient indexClientForQueries = CreateSearchIndexClient();\n",
    "\t     // Call the search service and store the results\n",
    "            DocumentSearchResult results = await \t\t\t\n",
    "\t\tindexClientForQueries.Documents.SearchAsync(searchText);\n",
    "            await SendResultsAsync(context, results);\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```csharp\n",
    " public ISearchIndexClient CreateSearchIndexClient()\n",
    "        {\n",
    "            string searchServiceName = “YourSearchService\";\n",
    "            string queryApiKey = “YourSearchKey\";\n",
    "            string indexName = “YourIndexName\";\n",
    "            SearchIndexClient indexClient = new SearchIndexClient(searchServiceName, indexName, new SearchCredentials(queryApiKey));\n",
    "            return indexClient;\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```csharp\n",
    " public async Task SendResultsAsync(ITurnContext context, DocumentSearchResult results)\n",
    "       {\n",
    "            IMessageActivity activity = context.Activity.CreateReply();\n",
    "            if (results.Results.Count == 0)\n",
    "            {\n",
    "                await SearchResponses.ReplyWithNoResults(context, searchText);\n",
    "            }\n",
    "            else // this means there was at least one hit for the search\n",
    "            {\n",
    "                // create the response with the result(s) and send to the user\n",
    "                SearchHitStyler searchHitStyler = new SearchHitStyler();\n",
    "                searchHitStyler.Apply(\n",
    "                    ref activity,\n",
    "                    \"Here are the results that I found:\",\n",
    "                    results.Results.Select(r => \n",
    "\t\t\t\tImageMapper.ToSearchHit(r)).ToList().AsReadOnly());\n",
    "                await context.SendActivity(activity);\n",
    "            }\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can see the full lab for building this bot [here](https://github.com/Azure/LearnAI-Bootcamp/tree/master/lab02.2-building_bots) and the full bootcamp for Emerging AI Developers [here](https://github.com/Azure/LearnAI-Bootcamp/blob/master/emergingaidev_bootcamp.md). You can also find more Azure Search samples [here](https://github.com/Azure-Samples/search-dotnet-getting-started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 2.4: Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The final sample we'll go through is good for those working with the BotBuilder v3 SDK for .NET. This [sample Computer Vision bot](https://github.com/Microsoft/BotBuilder-Samples/tree/master/CSharp/intelligence-ImageCaption) analyzes images and returns the caption to the user. We'll highlight a few pieces of the sample code.\n",
    "\n",
    "In the web.config file, you'll need to add your endpoint and key as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```html\n",
    " <appSettings>\n",
    "    <add key = \"MicrosoftVisionApiEndpoint\" value=\"PUT-API-ENDPOINT-HERE\" />\n",
    "    <add key = \"MicrosoftVisionApiKey\" value=\"PUT-YOUR-OWN-API-KEY-HERE\" />\n",
    " </appSettings>\n",
    "```\n",
    "You'll also need to create a class to store the results from your call to the Computer Vision API:\n",
    "```csharp\n",
    "namespace Microsoft.ProjectOxford.Vision.Contract\n",
    "{\n",
    "    public class AnalysisResult\n",
    "    {\n",
    "        public AnalysisResult();\n",
    "\n",
    "        public Adult Adult { get; set; }\n",
    "        public Category[] Categories { get; set; }\n",
    "        public Color Color { get; set; }\n",
    "        public Description Description { get; set; }\n",
    "        public Face[] Faces { get; set; }\n",
    "        public ImageType ImageType { get; set; }\n",
    "        public Metadata Metadata { get; set; }\n",
    "        public Guid RequestId { get; set; }\n",
    "        public Tag[] Tags { get; set; }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once you have those two pieces configured, you can send the image(s) to the Computer Vision API and return the caption to the user:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```csharp\n",
    "/// <summary>\n",
    "/// This method calls <see cref=\"IVisionServiceClient.AnalyzeImageAsync(Stream, string[])\"/> and\n",
    "/// returns the first caption from the returned <see cref=\"AnalysisResult.Description\"/>\n",
    "/// </summary>\n",
    "public async Task<string> GetCaptionAsync(Stream stream)\n",
    "{\n",
    "    var client = new VisionServiceClient(ApiKey);\n",
    "    var result = await client.AnalyzeImageAsync(stream, VisualFeatures);\n",
    "    return ProcessAnalysisResult(result);\n",
    "}\n",
    "\n",
    "private static string ProcessAnalysisResult(AnalysisResult result)\n",
    "{\n",
    "    string message = result?.Description?.Captions.FirstOrDefault()?.Text;\n",
    "\n",
    "    return string.IsNullOrEmpty(message) ?\n",
    "                \"Couldn't find a caption for this one\" :\n",
    "                \"I think it's \" + message;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can read more details about the Computer Vision bot, and even deploy it yourself, [here](https://github.com/Microsoft/BotBuilder-Samples/tree/master/CSharp/intelligence-ImageCaption)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Section 3: Other ways to make bots smarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 3.1: Managing state data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The [Bot Builder Framework enables your bot to store and retrieve state data](https://docs.microsoft.com/en-us/azure/bot-service/dotnet/bot-builder-dotnet-state) that is associated with a user, a conversation, or a specific user within the context of a specific conversation. State data can be used for many purposes, such as determining where the prior conversation left off or simply greeting a returning user by name. If you store a user's preferences, you can use that information to customize the conversation the next time you chat. For example, you might alert the user to a news article about a topic that interests her or alert a user when an appointment becomes available.\n",
    "\n",
    "In the Bot Builder v4 SDK, for each activity that your application receives, the adapter creates a [new context object](https://github.com/Microsoft/botbuilder-dotnet/wiki/Overview) that includes information about the channel, the sender, the conversation, and the received activity (you can see how the SDK does that [here](https://github.com/Microsoft/botbuilder-dotnet/tree/master/libraries/Microsoft.Bot.Builder.Core)). This context object exists only for the processing of that one activity. Your bot is otherwise stateless. So, for testing and prototyping purposes, this may be enough. For production bots, you can implement your own storage adapter or use one of the Azure extensions. The Azure extensions allow you to store your bot's state data in either Table Storage, CosmosDB, or SQL.\n",
    "\n",
    "Get into the code behind [Bot State](https://github.com/Microsoft/botbuilder-dotnet/blob/master/libraries/Microsoft.Bot.Builder.Core.Extensions/BotState.cs) and using [Table Storage](https://github.com/Microsoft/botbuilder-dotnet/tree/master/libraries/Microsoft.Bot.Builder.Azure) in the v4 SDK.  \n",
    "If you're developing with the v3 SDK, read about the best practices from the Bot Framework Team regarding [saving state data with BotBuilder-Azure in .NET](https://blog.botframework.com/2017/07/18/Saving-State-Azure-Extensions/).\n",
    "\n",
    "If you're developing bots (with any SDK), read more about [Sage's success orchestrating multiple bots with multilingual support](https://www.microsoft.com/developerblog/2017/01/21/orchestrating-multiple-bots-with-multilingual-support/). To speak briefly, Sage was supporting its localization and channel needs by writing multiple copies of the same bot for every language and every channel. Each of these bots needed to be registered and maintained separately - which resulted in a lot of code to maintain. Microsoft and Sage partnered together to architect a single bot that used middleware to enable context switching, which means that the user can switch between different child bots (e.g. Credit Card Bot and Health Checkup Bot), and to enable development with a multilingual approach using language resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Section 3.2: Dialogs vs Topics vs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many ways you can organize your code in bots. Ultimately, the decision will depend on the specifics of your project. When we were reviewing some of the aspects from the v4 SDK, a comment was made from the product group, \"The SDK allows you to organize code in whatever way you want.\" The SDK puts you, the developer, in control by design.  \n",
    "\n",
    "If you've developed or seen examples of v3 SDK bots, you'll notice that they're generally organized using Dialogs, where each Dialog, may represent the equivalent to a new screen in an application (see below and [read more](https://docs.microsoft.com/en-us/azure/bot-service/dotnet/bot-builder-dotnet-manage-conversation-flow)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![Dialogs vs Applications](./resources/assets/dialogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By the way, in case you were concerned about how to use Dialogs and FormFlow with the v4 SDK - [there’s a library for that](https://github.com/Microsoft/botbuilder-dotnet/tree/master/libraries/Microsoft.Bot.Builder.Classic) called \"Microsoft.Bot.Builder.Classic.\"\n",
    "\n",
    "In the v4 SDK, specifically the [AlarmBot sample](https://github.com/Microsoft/botbuilder-dotnet/tree/master/samples/AlarmBot), we explore a [MVVM style](https://msdn.microsoft.com/en-us/library/hh848246.aspx) of organizing code around topics. You can [read more](https://github.com/Microsoft/botbuilder-dotnet/tree/master/samples/AlarmBot) about how they implement it with an AlarmBot, but a brief description is provided below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Topics](./resources/assets/topics.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To provide an example, take the picture bot we referenced when we talked about integrating services into bots. We created the bot with the v3 SDK with .NET Framework and the v4 SDK with .NET Core, and below shows how the organization differed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Dialogs vs Topics](./resources/assets/dialogsvtopics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So you can see, for this example, the result is similar. The nice part about using Topics with the v4 SDK is all responses are separated from the logic within the topics, making the topics relatively clean and focused on directing the conversation.  \n",
    "\n",
    "At the end of the day, you should pick the organization that makes the most sense for your bot.\n",
    "\n",
    "You can see the full set of labs for building the picture bot [here](https://github.com/Azure/LearnAI-Bootcamp/blob/master/emergingaidev_bootcamp.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continue to [2_activity](./2_activity.md)\n",
    "Back to [README](./readme.md)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## References\n",
    "  \n",
    "- Cognitive Services\n",
    "    - [Cognitive Services](https://azure.microsoft.com/en-us/services/cognitive-services/)\n",
    "    - [Cognitive Services documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/Welcome)\n",
    "    - [Add intelligence to bots](https://docs.microsoft.com/en-us/azure/bot-service/bot-service-concept-intelligence)\n",
    "    - [LUIS](https://azure.microsoft.com/en-us/services/cognitive-services/language-understanding-intelligent-service/)\n",
    "    - [Text analytics](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/)\n",
    "    - [Bing spell check](https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/)\n",
    "    - [Text analytics models in AML Studio](https://docs.microsoft.com/en-us/azure/machine-learning/studio/text-analytics-module-tutorial)\n",
    "    - [Translator Text](https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/)\n",
    "    - [Project Entity Linking](https://labs.cognitive.microsoft.com/en-us/project-entity-linking)\n",
    "    - [Project Knowledge Exploration](https://labs.cognitive.microsoft.com/en-us/project-knowledge-exploration)\n",
    "    - [Project Academic Knowledge](https://labs.cognitive.microsoft.com/en-us/project-academic-knowledge)\n",
    "    - [QnA Maker](https://qnamaker.ai/)\n",
    "    - [Custom Decision Service](https://azure.microsoft.com/en-us/services/cognitive-services/custom-decision-service/)\n",
    "    - [Bing Speech](https://azure.microsoft.com/en-us/services/cognitive-services/speech/)\n",
    "    - [Custom Speech Service](https://azure.microsoft.com/en-us/services/cognitive-services/custom-speech-service/)\n",
    "    - [Speaker Recognition](https://azure.microsoft.com/en-us/services/cognitive-services/speaker-recognition/)\n",
    "    - [Translator Speech](https://azure.microsoft.com/en-us/services/cognitive-services/translator-speech-api/)\n",
    "    - [Bing web search](https://azure.microsoft.com/en-us/services/cognitive-services/bing-web-search-api/)\n",
    "    - [Bing image search](https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/)\n",
    "    - [Bing video search](https://azure.microsoft.com/en-us/services/cognitive-services/bing-video-search-api/)\n",
    "    - [Bing news search](https://azure.microsoft.com/en-us/services/cognitive-services/bing-news-search-api/)\n",
    "    - [Bing autosuggest](https://azure.microsoft.com/en-us/services/cognitive-services/autosuggest/)\n",
    "    - [Bing entity search](https://azure.microsoft.com/en-us/services/cognitive-services/bing-entity-search-api/)\n",
    "    - [Bing custom search](https://azure.microsoft.com/en-us/services/cognitive-services/bing-custom-search/)\n",
    "    - [Computer vision](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/)\n",
    "    - [Emotion](https://azure.microsoft.com/en-us/services/cognitive-services/emotion/)\n",
    "    - [Face](https://azure.microsoft.com/en-us/services/cognitive-services/face/)\n",
    "    - [Video indexer](https://azure.microsoft.com/en-us/services/cognitive-services/video-indexer/)\n",
    "    - [Custom vision](https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/)\n",
    "    - [Content moderator](https://azure.microsoft.com/en-us/services/cognitive-services/content-moderator/)  \n",
    "  \n",
    "- Adding Cognitive Services to bots\n",
    "    - [Lab for Azure Search and LUIS Samples](ttps://github.com/Azure/LearnAI-Bootcamp/tree/master/lab02.2-building_bots)\n",
    "    - [More Azure Search samples](https://github.com/Azure-Samples/search-dotnet-getting-started/blob/master/DotNetHowTo/DotNetHowTo/Program.cs)\n",
    "    - [QnA Maker sample](https://github.com/Microsoft/botbuilder-dotnet/tree/master/samples/Microsoft.Bot.Samples.Ai.QnA)\n",
    "    - [Sample Computer Vision bot](https://github.com/Microsoft/BotBuilder-Samples/tree/master/CSharp/intelligence-ImageCaption)\n",
    "  \n",
    "- Other ways to make bots smarter  \n",
    "  \n",
    "  Managing state data\n",
    "    - [Managing state data documentation](https://docs.microsoft.com/en-us/azure/bot-service/dotnet/bot-builder-dotnet-state)\n",
    "    - [Saving state data with the v3 SDK](https://blog.botframework.com/2017/07/18/Saving-State-Azure-Extensions/)\n",
    "    - [Middleware for managing state in the v4 SDK (.NET)](https://github.com/Microsoft/botbuilder-dotnet/blob/master/libraries/Microsoft.Bot.Builder.Core.Extensions/BotState.cs)\n",
    "    - [Example – Orchestrating multiple bots with multilingual support for Sage](https://www.microsoft.com/developerblog/2017/01/21/orchestrating-multiple-bots-with-multilingual-support/)  \n",
    "      \n",
    "  Dialogs vs Topics vs ?\n",
    "    - [Manage conversation flow](https://docs.microsoft.com/en-us/azure/bot-service/dotnet/bot-builder-dotnet-manage-conversation-flow)\n",
    "    - [Explanation of topics](https://github.com/Microsoft/botbuilder-dotnet/blob/master/samples/AlarmBot-Cards/README.md)\n",
    "    - [Want to use dialogs with SDK v4? There’s a library for that](https://github.com/Microsoft/botbuilder-dotnet/tree/master/libraries/Microsoft.Bot.Builder.Classic)  \n",
    "      \n",
    "  Middleware\n",
    "    - [Creating middleware with the v4 SDK](https://github.com/Microsoft/botbuilder-dotnet/wiki/Creating-Middleware)\n",
    "    - [Middleware and architecture for the v4 SDK](https://github.com/Microsoft/botbuilder-dotnet/wiki/Overview)\n",
    "    - [QnA maker documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/QnAMaker/Home)\n",
    "      \n",
    "- Additional resources  \n",
    "    - [LearnAI-Bootcamp](https://github.com/Azure/LearnAI-Bootcamp)\n",
    "    - [LearnAnalytics Website](http://learnanalytics.microsoft.com/)\n",
    "    - [AI Learning Paths](https://aischool.microsoft.com/learning-paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continue to [2_activity](./2_activity.md)\n",
    "Back to [README](./readme.md)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
